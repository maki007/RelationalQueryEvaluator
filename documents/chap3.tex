\chapter{Analysis}
\label{analysis}
Used data structures and algorithms in the implemented compiler are discussed in this chapter.
\section{Format of relational algebra}

In this section we present relation algebra operators which are the input of the compiler. Our relational algebra contains the following operators:
\begin{enumerate}
\item Projection -- we used extended projection $\pi_L$ which removes columns, computes new ones from expressions and renames attributes.

\item Table reading operator which is a leaf of the algebra tree. For this operator we need to provide the following arguments:
\begin{itemize}
\item table name.
\item information about indices (name, columns and sort order).
\item read columns.
\end{itemize}
\item Join - we used theta join $\Join_C$ operator where $C$ is condition having following format:
\begin{itemize}
\item Condition can be empty and in this case join represents Cartesian product.
\item $a_1=b_1~and~a_2=b_2~and~a_3=b_3~and...and~a_n=b_n$, where $a_k$ belongs to the first relation and $b_k$ belongs to the other relation.
\item $a_1\oplus b \ominus a_2$, where $a_1$ and $a_2$ belongs to one input and $b$ belongs to second input. Signs $\oplus$ and $\ominus$ mean $<$ or $\leq$.

\end{itemize}

In addition to condition, we need to specify output attributes of join. These attributes can come from both inputs and we can optionally assign them new name. Assigning new attribute name is useful when some of the attributes have the same name.

The other types of joins are not directly supported, but they can be replaced with the cross join followed by desired selection.
\item Anti join operator which was not presented with other join algorithms. Output of expression $R \ltimes_C S$ is relation with tuples from $R$, for which do not exist any tuple from $S$ that satisfies condition $C$. We can use join and anti join can be used to express outer join.
 
The anti join can replace difference operator. The expression $R-S$ equals $R \ltimes_C S$, where $C$ is condition that equates each pair of attributes of $R$ and $S$, which had have same name.
 
Including the anti join in our relation algebra eliminates the need of usage the outer join and the difference and our relational algebra si simpler.

Condition $C$ of anti join $R \ltimes_C S$ has the following format:
\begin{itemize}
\item $a_1=b_1~and~a_2=b_2~and~a_3=b_3~and...and~a_n=b_n$, where $a_k$ belong to first the relation and $b_k$ belongs to the other relation.
\end{itemize}
In every anti join we need to specify its output attributes with optional new name. The anti join can output only columns from first relation. 
\item Group operator $\gamma_L$, where L is non empty list of group attributes and aggregate functions. Supported aggregate functions are $min$, $max$, $sum$ and $count$. The function $avg$ is not supported but it can easily computed using $sum$ and $count$. All mentioned functions except $count$ take one input attribute and function $count$ has empty input. 

As mentioned before, group operator is more general version of the duplicate elimination which is not included in our algebra.
\item Sort operator $\tau_L$, where $L$ is a non empty list of attributes with sort directions.
\item Bag union $\cup$. Both input relations has to have same names and types of attributes. The set union can be computed using bag union and grouping operator for duplicate elimination.
\item Selection used in our algebra does not differ from selection from classical relational algebra.

\end{enumerate}

Designed relational algebra works with bags.

\section{Physical algorithms}

In this section we enumerate and describe compiler's output algorithms. We assume that execution environment has enough memory and physical operators do not have to store intermediate result on hard drive.

Here is a list on algorithms:
\begin{itemize}
\item \texttt{Filter} - this algorithm reads input tuples and outputs tuples satisfying given condition. Output does not have to be sorted the same way as input.
\item \texttt{Filter~keeping~order} - this algorithm reads input tuples and outputs tuples satisfying given condition. Output has to be sorted the same way as input.
\item \texttt{Hash~group} - operator groups tuples using hash table and for every group of tuples aggregate functions are computed.
\item \texttt{Sorted~group} - operator groups sorted tuples and computes aggregate functions. Input has to be sorted by group attributes.
\item \texttt{Column~operations} - this is an implementation of extended projection algebra operator. 
\item \texttt{Cross~join} - operator computes Cartesian product of two relations.
\item \texttt{Hash~join} - operator uses hash table to compute join of two relations $R$ and $S$ with condition $C$, where $C$ has the following format: $r_1=s_1 and r_2=s_2 and ... r_n=s_n$. Attributes $(r_1,r_2,...,r_n)$ belong to the relation $R$ and $(s_1,s_2,...,s_n)$ are from the relation $S$.
\item \texttt{Merge~equijoin} - algorithm takes advantage of sorted inputs to compute join with condition $C$, where $C$ has same format like condition in \texttt{Hash~join}. 
\item \texttt{Merge~non~equijoin} - operator computes theta join with condition $a_1\oplus b \ominus a_2$, where $a_1$ and $a_2$ belong the first input and $b$ belongs to the second input. Signs $\oplus$ and $\ominus$ means $<$ or $\leq$. Input relations has to be sorted by the attributes in the join condition.
\item \texttt{Hash~anti~join} -  algorithm computes anti join with condition $C$  using hash table. Condition $C$ have the same format like condition in \texttt{Hash~join}
\item \texttt{Merge~anti~join} - operator takes advantage of sorted inputs to compute anti join with condition $C$, where $C$ has same format like condition in \texttt{Hash~join}. 
\item \texttt{Table~scan} - operator scans whole table from hard drive.
\item \texttt{Scan~and~sort~by~index} - operator scans whole table from hard drive using index. Output will be sorted by columns on used index.
\item \texttt{Index~Scan} - this algorithm uses index to read tuples from table satisfying given condition.
\item \texttt{Sort} - this algorithm sorts input. Input can be presorted and in this case operator sorts only by not yet sorted attributes.
\item \texttt{Union} - algorithm is an implementation of bag union.

\end{itemize}


\section{Architecture}
The architecture of implemented tool is displayed in the figure~\ref{fig:compilerarchitecture}.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{compilerarchitecture}

      \caption{Compiler architecture.}
          \label{fig:compilerarchitecture}
\end{figure}

The relational algebra tree is read from XML file. For this format we decided for the following reasons:
\begin{itemize}
\item XML has tree--like structure.
\item For validation we only need to write schema.
\item There are already implemented tools for parsing.
\item There is no need to write input parser.
\end{itemize}

The relational algebra tree is checked in the \texttt{Semantic analyzer}. This component checks if all of used the attributes are in the input relation. Semantic analyzer searches for duplicate named attributes and reports it as an error. 

Semantically correct tree is processed by component \texttt{Algebra grouper} that groups neighboring joins into one. Thanks to this operation so we can later choose fastest way to join multiple relations.

Algebra tree with grouped joins is optimized. We implemented one most important optimization: pushing selections down the tree. This component pushes selections to the join node if selection contains condition $C$ where $C$ is in the following format $a=b$. Attribute $a$ is from the first join relation and $b$ belongs to the second join relation.

Optimized algebra tree is processed by compiler, which generates physical plan, which is not final. Its sort operator's parameters does not have to be final. For example if we want to sort relation before grouping we can sort it in different directions and than later decide what direction is better.

Final plan is an output of the component named \texttt{Sort resolver}. This component resolved unknown sort order of sort operators and produces final plan, which is converted too Bobolang language.

Implemented tool does not check types because it will be the back end of the compiler. We assume that the front end parsing the text will handle types. Types of columns are only copied to the output and we assume that column types do not contain any errors.

\section{Data structures}

Data structures used in implementing tool are presented in this section.

Relational algebra si stored in polymorphic tree. Every node stores its parameters pointer on parent in the tree and zero or more pointers on children node. No other structure was considered for this representation since this is efficient way to store logical plan. It allows easily to add new types of relational algebra operators and it is not hard to manipulate with the tree. We can remove or add new node very easily. 
Example of this representation can be found in figure \ref{fig:groupalgebra}. It's representing simple query reading whole table, then grouping it and computing some aggregation functions. The result is sorted at the end. Leaf of the tree also stores some information about indexes on read table, list of columns with their types and number of unique values. Other important parameter is size of relation which is displayed in number of rows parameter.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{groupalgebra}

      \caption{Example of relational algebra structure.}
          \label{fig:groupalgebra}
\end{figure}

We choose same structure for physical plan. Physical plan usually doesn't have to change. The advantage of storing it into polymorphic tree is to ability to easily add new root node. Example of this representation can be found in figure \ref{fig:groupplan}. This figure contains one of possible physical plans for relational algebra in figure  \ref{fig:groupalgebra}. For reading we used algorithm table scan, then we hashed input by requested columns and at the end, we sorted it using sort operator. Every nodes stores additional information like output attributes, estimated time and size of output relation.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.6\textwidth]{groupplan}

      \caption{Example of physical plans structure.}
          \label{fig:groupplan}
\end{figure}

Physical and logical plan also contain expressions. Expressions are stored in polymorphic expression tree. We have example of this structure in figure~\ref*{fig:expressiontree}. This structure represents expression $X*Y+874$.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.6\textwidth]{expressiontree}

      \caption{Example expression tree.}
          \label{fig:expressiontree}
\end{figure}

More complicated structure was used to storing parameters. This structure is stored in every sort physical operator to determine what columns should relation be sorted by. 

If we want to use group operator based on sort and it groups by three columns, we don't know which sort direction to use. Let's have expression $\gamma_{x,y}(R)$. There is four way to sort expression before calling group operator. This ways are:
\begin{itemize}
\item $x:A,y:A$
\item $x:A,y:D$
\item $x:D,y:A$
\item $x:D,y:D$
\end{itemize}
$A$ means ascending and $D$ is abbreviation for descending.

If we want to use merge join, joining on two attributes, we don't know direction and also which column should be first and which second. For example let's have $R\Join_{r_1=s_1~and~r_2=s_2} S$. In this case we can sort relation $R$ following way:
\begin{itemize}
\item $r_1,r_2$
\item $r_2,r_1$
\end{itemize}
Order, how to sort columns is also unknown.

We also want to store information about equality of sort column. After merge join $R\Join_{r_1=s_1} S$ is result sorted by $r_1$ or $s_1$.

All this requirements were use to design structure to store sort parameter without enumerating all possible sort orders.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.9\textwidth]{sortparameters}

      \caption{Structure storing parameters for sort.}
          \label{fig:sortparameters}
\end{figure}

In figure \ref{fig:sortparameters} we display an example of sort parameters, which sorts by 6 columns. It usually contains from 1 or more columns group. The order of columns groups cannot be changed. Order of columns in groups is arbitrary. It means that $F$ has to be on sixth place, but column $E$ can be on forth of fifth. Every column contains information about sort order: $ASC$ (ascending), $Desc$ (descending) or $UNK$ (unknown - can be ascending or descending). Every column also can be list of attributes which are equal to it. If we for example in projection remove attribute $A$, we still have attributes $X$, $Y$ and $Z$ which are equal to it, so one can take it's place.

Figure \ref{fig:sortparameters} represents many sort order possibilities we enumerate only some of them:
\begin{enumerate}
\item $A:ASC,C:DESC,B:ASC,H:DESC,D:ASC,F:DESC$
\item $C:DESC,B:ASC,Z:ASC,H:DESC,D:DESC,F:DESC$
\item $B:ASC,C:DESC,A:ASC,E:ASC,D:DESC,F:DESC$
\item $C:DESC,B:ASC,Y:ASC,D:ASC,H:ASC,F:DESC$
\end{enumerate}



\section{Optimization}

In this section we describe algebra optimization, which wes implemented to improve logical plan.

Before we start with optimizations we need to prepare logical plan for it. We group joins algebra nodes and expressions connected with $and$ and $or$.

Basically we go from top of the tree. If we find join we convert to it grouped join. If on of it's children is join we merge it. This representation is used for choosing faster way to order join. We do the same thing for conditions. From expression tree $a=2~and~(b=2~and~c=2)$ we create $AND(a=2,b=2,c=2)$. This representation is useful for splitting condition into simpler conditions.

We implemented very important optimization: pushing selection down the tree. Every selection is spitted into selections with simpler conditions. For every such selection we try to move it down the tree as much as possible. In this phase we used following rules ($\sigma_C$ is being pushed down):
\begin{enumerate}
\item $\sigma_C(\sigma_D(R))=\sigma_D(\sigma_C(R))$
\item $\sigma_C(\pi_L(R))=\pi_L(\sigma_C(R))$, it works only if $C$ doesn't contains new computed columns in extended projection. We also need to rename columns in condition $C$ in case there was some renaming.
\item  $\sigma_C(R \Join_D S)$ can be rewritten as
\begin{enumerate}
\item $\sigma_C(R) \Join_D S$ if $C$ contains only columns from $R$.
\item $R \Join_D \sigma_C(S)$ if $C$ contains only columns from $R$.
\item $R \Join_{D~and~C} S$ if $C$ is in form $a=b$ where $a$ is from $R$ and $b$ is from $S$.
\end{enumerate}
\item $\sigma_C(R \ltimes_C S)=\sigma_C(R)\ltimes_C S$ if $C$  contains only columns from $R$, which is always because output of $R \ltimes_C S$ can contain only columns $R$.
\item $\sigma_C(R\cup S)=\sigma_C(R)\cup \sigma_C(S)$
\end{enumerate}

\section{Generating physical plan}

We try to choose easiest method for generating plans. The decision was between heuristic method and dynamic programming. From amount of code it was probably the same. We chose dynamic programming, because it can give better results. We just generate all possible plans for each node and choose the fastest.

We process logical plan from leafs. For every leafs we generate all possible physical algorithms and we insert resulting plans into heap, where we keep $c$ fastest plans for current node, where $c$ is constant set in compiler.

For every node we use plans generated in it's children to generate new plan. This way we go up the logical plan tree up to the root.

For comparing physical plans we estimated run time. For every node we store how long it will run. Estimated time for plan is sum of estimated times of all nodes.

Very important are equations, which compute estimated time for nodes. They depend from size of input relation. Modifying them can resolve in getting better physical plans. For example if physical algorithm hash join takes too much time because of lot random accessing memory, we can modify estimated time so sort with merge join will be preferred.

Crucial are informations about size of tables. If they are not in input we use default value and physical plan will be probably worse. Other important parameter is number of unique values in table column. Size of join depends on it and since joins usually takes significant amount of time, it is important to have as precise values as possible. 

\subsection{Join order selecting algorithm}


In section \ref{joinOrder} we presented algorithm choosing join order. We should choose this order and then assign join algorithms. We do it in one phase for following reason. In case we don't have information about table sizes, we cannot determine join order because all are the same. In this situation we can first join relations which are sorted some way to get a faster plan.

In this section we describe algorithm for selecting join order. We are using Two algorithms dynamic programming and greedy algorithm. For dynamic programming we use version where we enumerate all possible trees. This algorithm can give us very good plans but has exponential complexity and that's why we use it only if number of joined relations in grouped join node is small. If we join more relations we use greedy algorithm, which only generated left or right deep trees but it's complexity is not exponential only polynomial.

Input in both algorithm are set of plans for every input join relation.

\subsubsection{Dynamic programing for selection join order}

We use a variation of algorithm described in section \ref{dymanicalgorithm}. We number then input relations from $1$ to $n$. For actual computation we use table, where we store plans. Key of the table is non empty subset of set $1..n$.

For every table entry we only store $k$ best plans. It represents best plans for, that were created by joining input in key of table.

In begin we store input plans into table entry identified by set containing input number. In first iterator we fill tables entries, which key have two values by combining plans from entries with key size $1$.

Then we compute plans for entries, which have key size $3,4...n$. We do so by spiting set in key of the entry in all possible pairs of non empty disjunctive subsets. We take plans from this subsets and we generate new plans combining them. We only store $k$ fastest plans.

We do this until we compute plans for table entry ${1..n}$. This is our result.

Time complexity is at least exponential since we generate all subsets of $n$ relations, this number is $2^n$.


\subsubsection{Greedy algorithm for selection join order}
 This is also a variation of algorithm described in section \ref{greedyalgorithm}. It beginning we create joins of every pair of relations. From them we choose best $k$ trees.
 
 In every following iteration for every tree we generate new trees by adding new relation to tree. In following iteration we continue only with best $k$ join trees. At the end we have maximal $k$ plans.
 
 Time complexity is $O(n^2)$. In every iteration we generate from every tree maximal $n$ new trees, but we keep only $k$ of them for next iteration. Number of iteration is $n-1$, because all trees grow by one in every iteration. Number $k$ is a constant so it doesn't change time complexity.
 
\subsection{Resolving sort parameters}

After we generated physical plan we need to decide what sort parameters in sort operator to use.

To do this we go down the tree and store information about how relations is sorted. Based on that we adjust sort parameters or just choose arbitrary order if possible.

