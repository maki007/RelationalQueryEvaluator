\chapter{Related work}
\label{relatedwork}
The basics of the theory of relational algebra along with its optimizations and physical plan generation are introduced in this chapter. Information for this chapter and compiler implementation was taken from Database systems~\cite{database}.
\section{Relational algebra}

In this section we introduce and describe relational algebra\cite{database}. Following definitions of relational model can vary depending on used literature.
\begin{mydef}
$Relation$ is a two dimensional table.
\end{mydef}
\begin{mydef}
$Attribute$ is a column of a table.
\end{mydef}
\begin{mydef}
$Schema$ is the name of the relation and the set of its attributes. For example:~$Movie~(id,name,length)$.
\end{mydef}
\begin{mydef}
$Tuple$ is a row of a relation.
\end{mydef}

Relational algebra has atomic operands:
\begin{itemize}
\item Variables that are relations.
\item Constants which are relations. 
\end{itemize}

In classical relational algebra all operators and expression results are sets. All these operations can be applied also to bags(multi sets). We used relational algebra based on bags in the implemented compiler.

Classical relational algebra operators are:
\begin{itemize}
\item Set operations -- union, difference, intersection.
\item Removing operators -- selection, which removes rows and projection that eliminates columns from given relation.
\item Operations that combine two relations: all kinds of joins.
\item Renaming operations that do not change tuples of the relation but only its schema.
\end{itemize}
Expressions in relational algebra are called $queries$.
\subsection{Classical relational algebra operators}
\subsubsection{Set operations on relations}

Sets operations are:
\begin{itemize}
\item Union $R\cap_S S$ is a set of tuples that are in $R$ or $S$.
\item Intersection $R\cup_S S$  is a set of tuples that are both in $R$ and $S$. 
\item Difference $R-_S S$ is a set of tuples that are in $R$ but not in $S$.

\end{itemize}

Considering two relations $R$ and $S$, if one wants to apply some set operation, both relations must have the same set of attributes. We can also use renaming operations if relations do not have same attribute names.

\subsubsection{Projection}
\label{projection}
$Projection$ operator $\pi_S$ produces new relation with reduced set of attributes from relation $R$. Result of an expression $\pi_{(S) A_1,A_3,A_4,...,A_N}(R)$ is relation $R$ with attributes $A_1,A_3,A_4,...,A_N$. Set version of this operator also eliminates duplicate tuples.

\subsubsection{Selection}
If the operator selection $\sigma$ is applied on relation $R$ with condition $C$, a new relation with the same attributes and tuples, which satisfy given condition, is obtained, for example $\sigma_{A_1=4}(R)$.

\subsubsection{Cartesian product}
Cartesian product of two sets $R$ and $S$ creates a set of pairs by choosing the first element of pair to be any element from R and second element of pair to be any element of S. Cartesian product of relations is similar. We pair tuples from $R$ with all tuples from $S$. Relations $S$ and $R$ cannot have attributes with the same name  because some columns of expression $R\times S$ could have the same name.

\subsubsection{Natural joins}
The simplest join is called natural join of $R$ and $S$ ($R \Join S$). Let schema of $R$ be $R(r_1,r_2,..,r_n,c_1,c_2,...c_n)$ and schema of $S$ be  $S(s_1,s_2,..,s_n,c_1,c_2,...c_n)$. In natural join we pair tuple $r$ from relation $R$ to tuple $s$ from relation $S$ only if $r$ and $s$ agree on all attributes with the same name (in this case, $c_1,c_2,...c_n$).

\subsubsection{Theta joins}
Natural join forces us to use one specific condition. In many cases we want to join relations with some other condition. The theta join serves for this purpose. The notation for joining the relations $R$ and $S$ based on the condition $C$ is $R\Join_C S$. The result is constructed in the following way:

\begin{enumerate}
\item Make Cartesian product of $R$ and $S$.
\item Use selection with condition $C$.
\end{enumerate}

In principle, $R\Join_C S=\sigma_C(R \times S)$. Relations $R$ and $S$ have to have disjunct names of columns.

\subsubsection{Renaming}

In order to control name of attributes or relation name we have renaming operator $\rho_{A_1=R_1,A_2=R_2,...,A_n=R_n}(R)$. Result will have the same tuples as $R$ and attributes ($R_1,R_2,...,R_n)$ will be renamed as $(A_1,A_2,...,A_n)$.

\subsection{Relational operations on bags}

Commercial database systems are almost always based on bags (multiset). The only operations that behave differently are intersection, union, difference and projection.

\subsubsection{Union}
Bag union of $R \cup_B S$ adds all tuples from $S$ and $R$ together. If tuple $t$ appears $m$-times in $R$ and $n$-times in $S$, then in $R \cup_B S$ $t$ will appear $m+n$ times. Both $m$ and $n$ can be zero.

\subsubsection{Intersection}

Assume we have tuple $t$ that appears $m$-times in $R$ and $n$-times in $S$. In the bag intersection $R \cap_B S$ $t$ will be $min(m,n)$-times.

\subsubsection{Difference}
Every tuple $t$, that appears $m$-times in $R$ and $n$-times in $S$, will appear $max(0,m-n)$ times in bag $R-_B S$.

\subsubsection{Projection}
Bag version of projection $\pi_B$ behaves like set version $\pi_S$ with one exception. Bag version does not eliminate duplicate tuples.

\subsection{Extended operators of relational algebra}

We will introduce extended operators that proved useful in many query languages like SQL.


\subsubsection{Duplicate elimination}
Duplicate elimination operator $\delta(R)$ returns set consisting of one copy of every tuple that appears in bag $R$ one or more times.

\subsubsection{Aggregate operations}

Aggregate operators such as sum are not relational algebra operators but are used by grouping operators. They are applied on column and produce one number as a result. The standard operators are $SUM$, $AVG$(average), $MIN$, $MAX$ and $COUNT$.


\subsubsection{Grouping operator}

Usually, it is not desirable to compute aggregation function for the entire column, i.e.~one rather computes this function only for some group of columns. For example, average salary for every person can be computed or the people can be grouped by companies and the average salary in every company is obtained. 

For this purpose we have grouping operator $\gamma_L(R)$, where $L$ is a list of:

\begin{enumerate}
\item attributes of $R$ by which $R$ will be grouped
\item expression $x=f(y)$, where $x$ is new column name, $f$ is aggregation function and $y$ is attribute of relation. When we use function $COUNT$ we do not need to specify argument.
\end{enumerate}

Relation computed by expression $\gamma_L(R)$ is constructed in the following way:

\begin{enumerate}
\item Relation will be partitioned into groups. Every group contains all tuples which have the same value in all grouping attributes. If there are no grouping attributes, all tuples are in one group.
\item For each group, operator produces one tuple consisting of:
 \begin{enumerate}
 	\item Values of grouping attributes.
 	\item Results of aggregations over all tuples of processed group.
 \end{enumerate}
\end{enumerate}

Duplicate elimination operator is a special case of grouping operator. We can express $\delta(R)$ with $\gamma_{L}(R)$, where $L$ is a list of all attributes of $R$.

\subsubsection{Extended projection operator}

We can extend classical bag projection operator $\pi_L(R)$ introduced in Chapter~\ref{projection}. It is also denoted as $\pi_L(R)$ but projection list can have following elements:

\begin{enumerate}
\item Attribute of $R$, which means attribute will appear in output.
\item Expression $x = y$, attribute $y$ will be renamed to $x$.
\item Expression $x = E$, where $E$ is an expression created from attributes from~$R$, constants and arithmetic, string and other operators. The new attribute name is $x$, for example $x=e*(1-l)$.
\end{enumerate}



\subsubsection{The sorting operator}

In several situations we want the output of query to be sorted. Expression $\tau_L(R)$, where $R$ is relation and $L$ is list of attributes with additional information about sort order, is a relation with the same tuples like $R$ but with  different order of tuples. Example $\tau_{A_1:A,A_2:D}(R)$ will sort relation $R$ by attribute $A_1$ ascending and tuples with the same $A_1$ value will be additionally sorted by their $A_2$ value descending. Result of this operator is not bag or set  since there is no sort order defined in bags or sets. Result is sorted relation and it is essential to use this operator only on top of algebra tree.

\subsubsection{Outer joins}
Assuming join $R\Join_C S$, we call tuple $t$ from relation $R$ or $S$ $dangling$ if
we did not find any match in relation $S$ or $R$. Outer join $R\Join^\circ_C S$ is formed by creating $R\Join_C S$ and adding dangling tuples from $R$ and $S$. The added tuples must be filled with special $null$ value in all attributes they do not have but appear in the join result.

Left or right outer join is an outer join where only dangling tuples from left or right relation are added, respectively.

\section{Optimizations of relational algebra}

After generation of the initial logical query plan, some heuristics can be applied to improve it using some algebraic laws that hold for relational algebra. 


\subsection{Commutative and associative laws}
Commutative and associative operators are Cartesian product, natural join, union and intersection. Theta join is commutative but generally not associative. If the conditions make sense where they were positioned, then theta join is associative.
This implies that one can make following changes to algebra tree:

\begin{itemize}
\item $R \oplus S = S \oplus R$

\item $(R \oplus S) \oplus T = R \oplus (S \oplus T)$,
\end{itemize}
where $\oplus$ stands for $\times$, $\cap$, $\cup$, $\Join$ or $\Join_C$.

\subsection{Laws involving selection}

Selections are important for improving logical plan. Since they usually reduce the size of relation markedly we need to move them down the tree as much as possible.
We can change order of selections:

\begin{itemize}
\item $\sigma_{C_1}(\sigma_C{_2}(R)) = \sigma_C{_2}(\sigma_C{_1}(R))$
\end{itemize}
Sometimes we cannot push the whole selection but we can split it:

\begin{itemize}
\item $\sigma_{C_1~AND~C_2}(R)=\sigma_{C_1}(\sigma_{C_2}(R))$

\item $\sigma_{C_1~OR~C_2}(R)=\sigma_{C_1}(R) \cup_S \sigma_{C_2}(R)$
\end{itemize}
Last law works only when $R$ is a set. 

When pushing selection through the union, it has to be pushed to both branches:
\begin{itemize}
\item $\sigma_{C}(R \cup S)=\sigma_{C}(R) \cup \sigma_{C}(S)$
\end{itemize}

When pushing selection through the difference, we must push it to the first branch. Pushing to the second branch is optional. Laws for difference are: 
\begin{itemize}
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - \sigma_{C}(S)$
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - S$
\end{itemize}

The following laws allows to push selection down the both arguments. Assuming the selection $\sigma_C$, it can pushed to the branch, which contains all attributes used in $C$. If $C$ contains only attributes of $R$, then
\begin{itemize}
\item $\sigma_{C}(R \oplus S)=\sigma_{C}(R) \oplus S$,
\end{itemize}
where $\oplus$ stands for $\times$, $\Join$ or $\Join_C$. If relations $S$ and $R$ contain all attributes of $C$, the following law can be also used:
\begin{itemize}
\item $\sigma_{C}(R \Join S)=\sigma_{C}(R) \Join  \sigma_{C}(S)$
\end{itemize}


\subsection{Laws involving projection}
We can add projection anywhere in the tree as long as it only eliminates attributes, which are not used anymore and they are not in query result.

\subsection{Laws involving joins and products}

We have more laws involving selection that follow directly from the definition of the join:
\begin{itemize}
\item $\sigma_{C}(R \times S)=R \Join_{C} S$
\item $R \Join S=\pi_L(\sigma_{C}(\pi_X(R) \times \pi_Y(S)))$. $\pi_A(B)$ renames all attributes of relation $B$ from $attributename$ to $B\_attributename$, where $A\in\{X,Y\}$ and $B\in\{R,S\}$. $C$ is condition that equates each pair of attributes of $R$ and $S$, which had the same name before renaming. $\pi_L$ keeps all columns not used in condition $C$ and renames them back by removing prefix $S\_$ or $R\_$. It also keeps all columns used in $C$ which came from relation $R$ and renames them by removing prefix~$R\_$.
\end{itemize}



\section{Physical plan generation}

Physical plan will be created from optimized logical plan. We generate many physical plans and choose the one with the least estimated cost (run time). This approach is called cost--based enumeration.

For every physical plan we select:
\begin{enumerate}
\item an order of grouping and joins.
\item an algorithm for each operator, for example, usage of join based on hashing or sorting.
\item additional operators which are not present in logical plan, for example, we can sort relation in order to use faster algorithm which assumes that its input is sorted.
\item the way in which arguments are passed between operators. We can use iterators  or store the result on the hard drive.
\end{enumerate}

\subsection{Size estimations}
Estimates used in this section are taken from Database systems~\cite{database}. 
The costs of evaluation of physical plan are based on the estimated size of interme\-dia\-te relations. Ideally, we want our estimation to be accurate, easy to compute and logically consistent (size of relation does not depend on how relation is computed). We will present simple rules, which will give us good estimates in most situations. The goal of estimation of sizes is not to predict the exact size of relation as even an inaccurate size will help us with plan generation.

In this section we will use the following conventions:

\begin{itemize}
\item $T(R)$ is the number of tuples in relation R.
\item $V(R,a)$ is the number of distinct values in attribute $a$. 
\item $V(R,[a_1,a_2,...,a_n])$ is the number of tuples in $\delta(\pi_{a_1,a_2,...,a_n}(R))$
\end{itemize}

\subsubsection{Estimating the size of a projection}

Bag projection is the only operator for which the size of its result is computable. It does not change the number of tuples but only their lengths.

\subsubsection{Estimating the size of a selection}

Selection usually reduces the number of tuples. For selection $S=\sigma_{A=c}(R)$, where $A$ is an attribute of $R$ and $c$ is a constant, we can use the following estimation:
\begin{itemize}
\item $T(S)=T(R)/V(R,A)$
\end{itemize}

The selection involving inequality comparison, $S=\sigma_{A<c}(R)$, is more problematic. On average, half of the tuples satisfy the condition $A<c$ but usually, queries select only a small fraction of them from the relation. Typical inequality will return about a third of the original tuples. Therefore, the recommended estimate is:
\begin{itemize}
\item $T(S)=T(R)/3$
\end{itemize}

Selection with condition in the form $C_1~and~C_2~and~...~and~C_N$ can be treated as a cascade of simple selections and we can use the estimated size from simpler conditions to compute original selection estimate.

In case we have the condition $S=\sigma_{not(C)}(R)$, the following estimate is recommended:
\begin{itemize}
\item $T(S)=T(R)-T(\sigma_C(R))$
\end{itemize}

Estimates for selections with conditions involving logical disjunction are more complicated. Recommended estimated size of relation $S=\sigma_{C_1~or~C_2}(R)$ is:

\begin{itemize}
\item $T(S)=T(R)(1-(1-\dfrac{m_1}{T(R)})(1-\dfrac{m_2}{T(R)}))$
\end{itemize}

In order to use this estimate, we assume that conditions $C_1$ and $C_2$ are independent.
Variable $m_i$ equals $T(\sigma_{C_1}(R))$, where $i\in\{1,2\}$. Expression $1-m_1/T(R)$ is the fraction of tuples which do not satisfy the condition $C_1$ and $1-m_2/T(R)$ is the fraction of tuples which do not satisfy the  condition $C_2$. Product of these numbers is the fraction of tuples from $R$ which are not included in the result. The fraction of tuples in $S$ is given by the subtraction of the product from unity.

\subsubsection{Estimating the size of a join}
\label{join}
Considering the case of the natural join $A=R(X,Y)\Join S(Y,Z)$, we use the~following estimate:

\begin{itemize}
\item $T(A)=\dfrac{T(R)T(S)}{max(V(R,Y),V(S,Y))}$
\end{itemize}

We can generalize this rule for joining with multiple attributes. For join $R\Join S$, where we join $R$ and $S$ using the attributes $(a_1,a_2,...a_n)$, we use this estimate:

\begin{itemize}
\item $T(R\Join S)=\dfrac{T(R)T(S)}{\prod_{k=1}^{n}{max(V(R,a_k),V(S,a_k))}}$
\end{itemize}

When we join using multiple attributes, then the result of this estimate can be smaller than one. Such estimate indicates that the relation will be very small or possibly empty.

If we consider other types of join, e.g.~theta join, we can use the following rules for their estimation:
\begin{enumerate}
\item The size of the Cartesian product is the product of sizes of relations involved.
\item Equality conditions can be estimated using techniques presented for natural joins.
\item An inequality comparison can be handled like expression $\sigma_{A<c}(R)$. We assume that one third of the tuples will satisfy the condition. 
\end{enumerate}

\subsubsection{Estimating the size of an union}

Estimated size of expression $R \cup_B S$ is a sum of sizes of the relations $R$ and $S$. Size of the set union $R \cup_S S$ comes from the interval $\langle max(T(R),T(S)),T(R)+T(S)\rangle$. Recommended estimate is the midpoint of given interval.

\subsubsection{Estimating the size of intersection}
Size of relations $R\cap_S S$ and $R\cap_B S$ can be between $0$ and $min(T(R),T(S))$. Recommend estimate is to take half of the size of smaller relation:
\begin{itemize}
\item  $min(T(R),T(S))/2$.
\end{itemize}

\subsubsection{Estimating the size of a difference}

Result of expression $R-S$ can be as big as $T(R)$ and as small as $T(R)-T(S)$. Following estimate can be used for bag or set version of difference operator: 
\begin{itemize}
\item  $T(R-S)=T(R)-\frac{T(S)}{2}$.
\end{itemize}
\subsubsection{Estimating the size of a grouping}
Size of the result of expression $\gamma_L(R)$ is $V(R,[g_1,g_2,...,g_n])$, where $g_x$ are grouping attributes of $L$. This statistic is almost never available and for this case we need another estimate. Size of $\gamma_L(R)$ can be between 1 and $\prod_{k=1}^{n}{V(R,g_k)}$. Suggested estimate is:
\begin{itemize}
\item $T(\gamma_L(R))=min(\dfrac{T(R)}{2},\prod_{k=1}^{n}{V(R,g_k)})$
\end{itemize}
Estimation of duplicate elimination can be handled exactly like grouping.

\subsection{Enumerating plans}
For every logical plan there is an exponential number of physical plans. This section presents ways to enumerate physical plans such that the plan with the least estimated cost can be chosen. There are two broad approaches:
\begin{itemize}
\item Top--down: We work down the algebra tree from the root. For each possible implementation for algebra operation at the root, we consider best possible algorithms to evaluate its subtrees. We compute costs of every combination and choose the best one.
\item Bottom--up: We proceed up the algebra tree and we enumerate plans based on the plans generated for its subexpressions.
\end{itemize}
These approaches do not differ considerably. However, one method eliminates plans that other method cannot and vice versa.
\subsubsection{Heuristic selection}

We use the same approach for generating physical plan as we used to improve logical plan. We make choices based on heuristics, for example:

\begin{itemize}

\item If a join attribute has an index, we can use joining by this index.
\item If one argument of join is sorted, then we prefer using the merge join algorithm to the hash join algorithm.
\item If we compute intersection or union of more than two relations, we process smaller relations first.
\end{itemize}

\subsubsection{Branch--and--bound plan enumeration}
Branch--and--bound plan enumeration is often used in practice. We begin by finding physical plan using heuristics and denote its cost $C$. We can consider other plans for sub--queries. We can eliminate any plan for sub--query with cost greater than~$C$. If we get plan with lower estimated cost than $C$, we use this plan instead.

The advantage is that we can choose when to stop searchings for a better plan. If~$C$ is small, then we do not continue looking for other plan. On the other hand, when~$C$ is large, it is better to invest some time in finding a faster plan.
\subsubsection{Hill climbing}

First, we start with heuristically selected plan. Afterwards, we try to do some changes, for example, change of the order of joins or replacement of the operator using hash table for sort--based operator. Provided that such a plan is found, that no small modification results in a better plan, we choose that physical plan.

\subsubsection{Dynamic programming}
Dynamic programming is a variation of bottom--up strategy. For each subexpression, we keep only the plan with the least estimated cost.

\subsubsection{Selinger--Style Optimization}
Selinger--Style Optimization is an improvement of dynamic programming approach. For every subexpression, we keep not only the best plan, but also other plans with higher costs, the results of which are sorted in some way. This might be useful in the following situations:
\begin{enumerate}
\item The sort operator $\tau_L$ is on the root of tree and we have a plan, which is sorted by some or all attributes in $L$.
\item Plan is sorted by attribute used later in grouping.
\item We are joining by some sorted attribute.
\end{enumerate}
In this situations, either we do not sort the input or we use only partial sort. This way, we can use faster algorithms which takes advantage of the sorted input.


\subsection{Choosing the join order}
\label{joinOrder}
 We have three choices how to choose the order of joins of multiple relations:
\begin{enumerate}
\item Consider all possible options.
\item Consider only a subset of join orders.
\item Pick one heuristically.
\end{enumerate}
 
Algorithms, that can be used for choosing join order, are presented in this section.
 
\subsubsection{Dynamic programming algorithm}
\label{dymanicalgorithm}
Dynamic programming algorithm requires the use of a table for storage of the following information:
 
\begin{enumerate}
\item Estimated size of a relation.
\item Cost to compute current relations.
\item Expression of the way how the current relation was computed.
\end{enumerate}

Every input relation with estimated cost $0$ is stored in the table. For every pair of relations, we compute their estimated size and cost of join and store it in the table, see Section~\ref{join}.  

The next step is the computation of join trees of sizes $(3,4,...n)$.
If we want to consider all possible trees, we need to divide relations in current join $R$ into two non--empty disjoint sets. For each pair of sets, we compute the estimated size and cost of their join to get the join $R$. We use data already stored in our table for this purpose. The join tree with least estimated cost will be stored in the table. When we are estimating joins of $k$ relations, then all joins of $k-1$ relations must have been already estimated. 

Example: for estimating join $A\Join B\Join C\Join D$ we try to join the following trees:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\item $A\Join B$ and $C\Join D$
\item $A\Join C$ and $B\Join D$
\item $A\Join D$ and $B\Join C$

\end{enumerate}

 
Dynamic programming algorithm does not have to enumerate all possible trees, but only left--deep trees. A tree is called left--deep if all of its right children are leafs. Right--deep tree is tree, where all left children are leafs.

Computation differs only in division of processed relation $R$ in two non--empty disjoint subsets. One of the subsets can contain only one relation. 

In order to estimate join $A\Join B\Join C\Join D$, we try to join the following subsets:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\end{enumerate}

\subsubsection{Greedy algorithm}
\label{greedyalgorithm}
Time complexity of using dynamic programming to select an order of joins is exponential. Thus, we can use it only for small amounts of relations. If less time consuming algorithm is  desired, one can use faster greedy algorithm.
However, the disadvantage of greedy algorithm is that in some cases it outputs slower plan then dynamic programming algorithm.

 Greedy algorithm stores the same information as the dynamic programming algorithm for every relation in the table. We start with a pair of relations $R_i, R_j$, for which the cost of $R_i \Join R_j$ is the smallest. We denote this join as our current tree. For other relations, that are not yet included, we find a relation $R_k$, such that  its join with the current tree gives us the smallest estimated cost. We continue until all relations in our current tree are included. Greedy algorithm will create left--deep or right--deep join tree.


 
\subsection{Choosing physical algorithms}

To complete a physical plan we need to assign physical algorithms to operations in the logical plan.
\subsubsection{Choosing selection algorithms}

We try to use index for scanning table instead of reading the whole table. We can use the following heuristics for picking selection algorithm: 
\begin{itemize}
\item For selection $\sigma_{A\oplus c}$, relation $R$ which has index on attribute $A$, and constant $c$, we scan by index instead of scanning the whole table and filtering the result. Sign $\oplus$ denotes $=$, $<$, $\leq$, $>$ or $\geq$.
\item More generally, for selections containing condition $A\oplus c$ and selected relation with index on columns $A$, we can scan by index and filter the result by other parts of condition.
\end{itemize}

\subsubsection{Choosing join algorithms}

We recommend to use the sort join when:

\begin{enumerate}
\item One or both join relations are sorted by join attributes.
\item There are more joins on the same attribute. For join $R(a,b)\Join S(a,c) \Join T(a,d)$ we can use  sort based algorithm to join $R$ and $S$. If we assume that $R \Join S$ will be sorted by attribute $a$, then we use second sort join for $R\Join S$ and relation $T$.

\end{enumerate}

If we have join $R(a,b)\Join S(b,c)$ and we expect the size of relation to be small and $S$ has index on attribute $b$, we can use join algorithm using this index.
If there are no sorted relations and we do not have any indices on any relations, it is probably the best option to use the hash based algorithm.

\subsubsection{Choosing scanning algorithms}

Leaf of algebra tree will be replaced by one of scanning operators:

\begin{itemize}
\item $TableScan(R)$ -- operator reads the entire table.

\item $SortScan(R,L)$ -- operator reads the entire table and sorts by attributes in the list~$L$.

\item $IndexScan(R,C)$ -- $C$ is a condition in form $A\oplus c$, where $c$ is a  constant, $A$ is an attribute and $\oplus$ stands for $=$, $<$, $\leq$, $>$, $\geq$. Operator reads tuples satisfying condition $C$ using an index on $A$ and the result is sorted by columns on the used index.

\item $TableScan(R,A)$ -- $A$ is an attribute. Operator reads the entire table using an index on $A$ and the result will be sorted by columns on the used index.

\end{itemize}

We choose the scan algorithm based on the need of sorted output and availability of indices.

\subsubsection{Other algorithms}

Usually, sort and hash versions of algorithm are used. The following rules can be used for replacement of algebra operator with physical algorithm:

\begin{itemize}
\item We use the hash version of algorithm if the input is not sorted in a way we need or if the output does not have to be sorted.

\item We use the sort version of algorithm if we have the input sorted by some of requested parameters or we need sorted output. In case the input is only sorted by some of the needed attributes, we can still use the fast partial sort and apply sort based algorithm.

\end{itemize}