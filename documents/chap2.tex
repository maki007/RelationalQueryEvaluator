\chapter{Related work}
The basics of the theory of relational algebra along with its optimizations and physical plan generation are introduced in this chapter. Information for this chapter and compiler implementation was taken from Database systems~\cite{database}.
\section{Relational algebra}

In this section we introduce and describe relational algebra\cite{database}. Following definitions of relational model can vary depending on used literature.
\begin{mydef}
$Relation$ is a two dimensional table.
\end{mydef}
\begin{mydef}
$Attribute$ is a column of a table.
\end{mydef}
\begin{mydef}
$Schema$ is the name of the relation and the set of its attributes. For example:~$Movie~(id,name,length)$.
\end{mydef}
\begin{mydef}
$Tuple$ is a row of a relation.
\end{mydef}

Relational algebra has atomic operands:
\begin{itemize}
\item Variables that are relations.
\item Constants which are relations. 
\end{itemize}

In classical relational algebra all operators and expression results are sets. All these operations can be applied also to bags(multi sets). We used relational algebra based on bags in the implemented compiler.

Classical relational algebra operators are:
\begin{itemize}
\item Set operations -- union, difference, intersection.
\item Removing operators -- selection, which removes rows and projection that eliminates columns from given relation.
\item Operations that combine two relations: all kinds of joins.
\item Renaming operations that do not change tuples of the relation but only its schema.
\end{itemize}
Expressions in relational algebra are called $queries$.
\subsection{Classical relational algebra operators}
\subsubsection{Set operations on relations}

Sets operations are:
\begin{itemize}
\item Union $R\cap_S S$ is a set of tuples that are in $R$ or $S$.
\item Intersection $R\cup_S S$  is a set of tuples that are both in $R$ and $S$. 
\item Difference $R-_S S$ is a set of tuples that are in $R$ but not in $S$.

\end{itemize}

Considering two relations $R$ and $S$, if one wants to apply some set operation, both relations must have the same set of attributes. We can also use renaming operations if relations do not have same attribute names.

\subsubsection{Projection}
\label{projection}
$Projection$ operator $\pi_S$ produces new relation with reduced set of attributes from relation $R$. Result of an expression $\pi_{(S) A_1,A_3,A_4,...,A_N}(R)$ is relation $R$ with attributes $A_1,A_3,A_4,...,A_N$. Set version of this operator also eliminates duplicate tuples.

\subsubsection{Selection}
If the operator selection $\sigma$ is applied on relation $R$ with condition $C$, a new relation with the same attributes and tuples, which satisfy given condition, is obtained, for example $\sigma_{A_1=4}(R)$.

\subsubsection{Cartesian product}
Cartesian product of two sets $R$ and $S$ creates a set of pairs by choosing the first element of pair to be any element from R and second element of pair to be any element of S. Cartesian product of relations is similar. We pair tuples from $R$ with all tuples from $S$. Relations $S$ and $R$ cannot have attributes with the same name  because some columns of expression $R\times S$ could have the same name.

\subsubsection{Natural joins}
The simplest join is called natural join of $R$ and $S$ ($R \Join S$). Let schema of $R$ be $R(r_1,r_2,..,r_n,c_1,c_2,...c_n)$ and schema of $S$ be  $S(s_1,s_2,..,s_n,c_1,c_2,...c_n)$. In natural join we pair tuple $r$ from relation $R$ to tuple $s$ from relation $S$ only if $r$ and $s$ agree on all attributes with the same name (in this case, $c_1,c_2,...c_n$).

\subsubsection{Theta joins}
Natural join forces us to use one specific condition. In many cases we want to join relations with some other condition. The theta join serves for this purpose. The notation for joining the relations $R$ and $S$ based on the condition $C$ is $R\Join_C S$. The result is constructed in the following way:

\begin{enumerate}
\item Make Cartesian product of $R$ and $S$.
\item Use selection with condition $C$.
\end{enumerate}

In principle, $R\Join_C S=\sigma_C(R \times S)$. Relations $R$ and $S$ have to have disjunct names of columns.

\subsubsection{Renaming}

In order to control name of attributes or relation name we have renaming operator $\rho_{A_1=R_1,A_2=R_2,...,A_n=R_n}(R)$. Result will have the same tuples as $R$ and attributes ($R_1,R_2,...,R_n)$ will be renamed as $(A_1,A_2,...,A_n)$.

\subsection{Relational operations on bags}

Commercial database systems are almost always based on bags (multiset). The only operations that behave differently are intersection, union, difference and projection.

\subsubsection{Union}
Bag union of $R \cup_B S$ adds all tuples from $S$ and $R$ together. If tuple $t$ appears $m$-times in $R$ and $n$-times in $S$, then in $R \cup_B S$ $t$ will appear $m+n$ times. Both $m$ and $n$ can be zero.

\subsubsection{Intersection}

Assume we have tuple $t$ that appears $m$-times in $R$ and $n$-times in $S$. In the bag intersection $R \cap_B S$ $t$ will be $min(m,n)$-times.

\subsubsection{Difference}
Every tuple $t$, that appears $m$-times in $R$ and $n$-times in $S$, will appear $max(0,m-n)$ times in bag $R-_B S$.

\subsubsection{Projection}
Bag version of projection $\pi_B$ behaves like set version $\pi_S$ with one exception. Bag version does not eliminate duplicate tuples.

\subsection{Extended operators of relational algebra}

We will introduce extended operators that proved useful in many query languages like SQL.


\subsubsection{Duplicate elimination}
Duplicate elimination operator $\delta(R)$ returns set consisting of one copy of every tuple that appears in bag $R$ one or more times.

\subsubsection{Aggregate operations}

Aggregate operators such as sum are not relational algebra operators but are used by grouping operators. They are applied on column and produce one number as a result. The standard operators are $SUM$, $AVG$(average), $MIN$, $MAX$ and $COUNT$.


\subsubsection{Grouping operator}

Usually, it is not desirable to compute aggregation function for the entire column, i.e.~one rather computes this function only for some group of columns. For example, average salary for every person can be computed or the people can be grouped by companies and the average salary in every company is obtained. 

For this purpose we have grouping operator $\gamma_L(R)$, where $L$ is a list of:

\begin{enumerate}
\item attributes of $R$ by which $R$ will be grouped
\item expression $x=f(y)$, where $x$ is new column name, $f$ is aggregation function and $y$ is attribute of relation. When we use function $COUNT$ we do not need to specify argument.
\end{enumerate}

Relation computed by expression $\gamma_L(R)$ is constructed in the following way:

\begin{enumerate}
\item Relation will be partitioned into groups. Every group contains all tuples which have the same value in all grouping attributes. If there are no grouping attributes, all tuples are in one group.
\item For each group, operator produces one tuple consisting of:
 \begin{enumerate}
 	\item Values of grouping attributes.
 	\item Results of aggregations over all tuples of processed group.
 \end{enumerate}
\end{enumerate}

Duplicate elimination operator is a special case of grouping operator. We can express $\delta(R)$ with $\gamma_{L}(R)$, where $L$ is a list of all attributes of $R$.

\subsubsection{Extended projection operator}

We can extend classical bag projection operator $\pi_L(R)$ introduced in Chapter~\ref{projection}. It is also denoted as $\pi_L(R)$ but projection list can have following elements:

\begin{enumerate}
\item Attribute of $R$, which means attribute will appear in output.
\item Expression $x = y$, attribute $y$ will be renamed to $x$.
\item Expression $x = E$, where $E$ is an expression created from attributes from~$R$, constants and arithmetic, string and other operators. The new attribute name is $x$, for example $x=e*(1-l)$.
\end{enumerate}



\subsubsection{The sorting operator}

In several situations we want the output of query to be sorted. Expression $\tau_L(R)$, where $R$ is relation and $L$ is list of attributes with additional information about sort order, is a relation with the same tuples like $R$ but with  different order of tuples. Example $\tau_{A_1:A,A_2:D}(R)$ will sort relation $R$ by attribute $A_1$ ascending and tuples with the same $A_1$ value will be additionally sorted by their $A_2$ value descending. Result of this operator is not bag or set  since there is no sort order defined in bags or sets. Result is sorted relation and it is essential to use this operator only on top of algebra tree.

\subsubsection{Outer joins}
Assuming join $R\Join_C S$, we call tuple $t$ from relation $R$ or $S$ $dangling$ if
we did not find any match in relation $S$ or $R$. Outer join $R\Join^\circ_C S$ is formed by creating $R\Join_C S$ and adding dangling tuples from $R$ and $S$. The added tuples must be filled with special $null$ value in all attributes they do not have but appear in the join result.

Left or right outer join is an outer join where only dangling tuples from left or right relation are added, respectively.

\section{Optimizations of relational algebra}

After generation of the initial logical query plan, some heuristics can be applied to improve it using some algebraic laws that hold for relational algebra. 


\subsection{Commutative and associative laws}
Commutative and associative operators are Cartesian product, natural join, union and intersection. Theta join is commutative but generally not associative. If the conditions make sense where they were positioned, then theta join is associative.
This implies that one can make following changes to algebra tree:

\begin{itemize}
\item $R \oplus S = S \oplus R$

\item $(R \oplus S) \oplus T = R \oplus (S \oplus T)$,
\end{itemize}
where $\oplus$ stands for $\times$, $\cap$, $\cup$, $\Join$ or $\Join_C$.

\subsection{Laws involving selection}

Selections are important for improving logical plan. Since they usually reduce the size of relation markedly we need to move them down the tree as much as possible.
We can change order of selections:

\begin{itemize}
\item $\sigma_{C_1}(\sigma_C{_2}(R)) = \sigma_C{_2}(\sigma_C{_1}(R))$
\end{itemize}
Sometimes we cannot push the whole selection but we can split it:

\begin{itemize}
\item $\sigma_{C_1~AND~C_2}(R)=\sigma_{C_1}(\sigma_{C_2}(R))$

\item $\sigma_{C_1~OR~C_2}(R)=\sigma_{C_1}(R) \cup_S \sigma_{C_2}(R)$
\end{itemize}
Last law works only when $R$ is a set. 

When pushing selection through the union, it has to be pushed to both branches:
\begin{itemize}
\item $\sigma_{C}(R \cup S)=\sigma_{C}(R) \cup \sigma_{C}(S)$
\end{itemize}

When pushing selection through the difference, we must push it to the first branch. Pushing to the second branch is optional. Laws for difference are: 
\begin{itemize}
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - \sigma_{C}(S)$
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - S$
\end{itemize}

The following laws allows to push selection down the both arguments. Assuming the selection $\sigma_C$, it can pushed to the branch, which contains all attributes used in $C$. If $C$ contains only attributes of $R$, then
\begin{itemize}
\item $\sigma_{C}(R \oplus S)=\sigma_{C}(R) \oplus S$,
\end{itemize}
where $\oplus$ stands for $\times$, $\Join$ or $\Join_C$. If relations $S$ and $R$ contain all attributes of $C$, the following law can be also used:
\begin{itemize}
\item $\sigma_{C}(R \Join S)=\sigma_{C}(R) \Join  \sigma_{C}(S)$
\end{itemize}


\subsection{Laws involving projection}
We can add projection anywhere in the tree as long as it only eliminates attributes, which are not used anymore and they are not in query result.

\subsection{Laws involving joins and products}

We have more laws involving selection that follow directly from the definition of the join:
\begin{itemize}
\item $\sigma_{C}(R \times S)=R \Join_{C} S$
\item $R \Join S=\pi_L(\sigma_{C}(\pi_X(R) \times \pi_Y(S)))$. $\pi_A(B)$ renames all attributes of relation $B$ from $attributename$ to $B\_attributename$, where $A\in\{X,Y\}$ and $B\in\{R,S\}$. $C$ is condition that equates each pair of attributes of $R$ and $S$, which had the same name before renaming. $\pi_L$ keeps all columns not used in condition $C$ and renames them back by removing prefix $S\_$ or $R\_$. It also keeps all columns used in $C$ which came from relation $R$ and renames them by removing prefix~$R\_$.
\end{itemize}



\section{Physical plan generation}

After the optimization of the logical plan, the physical plan has to be created. We generate many physical plans and choose one with least estimated cost to run it. This approach is called cost-based enumeration.

For each physical plan we select 
\begin{enumerate}
\item An order of grouping and joins.
\item An algorithm for each operator. For example if we use join based on hashing or sorting.
\item Additional operators which are not presented in logical plan. For example we can sort relation in order to user faster algorithm which assumes that it's input is sorted.
\item The way in which arguments are pass to between operators. We can use iterators for it or store result on hard drive.
\end{enumerate}

\subsection{Size estimations}
Estimates used in this section are from book\cite{database}. 
The costs of evaluating physical plan are based of estimated size of interme\-dia\-te relations. Ideally we want our estimation to be accurate, easy to compute and logically consistent(size of relation doesn't depend on how relation is computed). We will present simple rules, which will give us give us good estimations in most situations. Goal of estimating sizes is not predict exact size of relation, even an inaccurate size will help us with plan generation.

In this section we will use following conventions:

\begin{itemize}
\item $T(R)$ is number of tuples in relation R.
\item $V(R,a)$ number of distinct values in attribute $a$. 
\item $V(R,[a_1,a_2,...,a_n])$ is number of tuples in $\delta(\pi_{a_1,a_2,...,a_n}(R))$
\end{itemize}

\subsubsection{Estimating the size of projection}

Bag projection is only operator which size of result is computable. It doesn't change number ot tuples, only their lengths change.

\subsubsection{Estimating the size of selection}

Selection usually reduces number of tuples. Let's have $S=\sigma_{A=c}(R)$, where $A$ is a attribute of $R$ and $c$ is a constant. Recommended estimation is:
\begin{itemize}
\item $T(S)=T(R)/V(R,A)$
\end{itemize}

More problematic estimation is when selection involves inequality comparison Let's have $S=\sigma_{A<c}(R)$. In average half the tuple satisfies condition, but usually queries select only a small fraction from all tuples. Typical inequality will return about third of tuples. Therefore the estimation is:
\begin{itemize}
\item $T(S)=T(R)/3$
\end{itemize}

For selection where condition is in form $C_1~and~C_2~and~...~and~C_N$ we can treat selection as a cascade of simple selections and estimate size for every simpler condition.

In case we have following condition: $S=\sigma_{not(C)}(R)$, we use this estimation:
\begin{itemize}
\item $T(S)=T(R)-T(\sigma_C(R))$
\end{itemize}

Little more complicated is when condition involves logical disjunction. Lets have expression $S=\sigma_{C_1~or~C_2}(R)$. We assume that $C_1$ and $C_2$ are independent. Size of $S$ is:

\begin{itemize}
\item $T(S)=T(R)(1-(1-\dfrac{m_1}{T(R)})(1-\dfrac{m_2}{T(R)}))$
\end{itemize}

$m_i=T(\sigma_{C_1}(R))$, where $i\in\{1,2\}$. Expression $1-\dfrac{m_1}{T(R)}$ is fraction of tuples which doesn't satisfy condition $C_1$ and $1-\dfrac{m_2}{T(R)}$ is fraction of tuples which doesn't satisfy condition $C_2$. Product of these numbers are the fraction of tuples from $R$ which are not in result. One minus the product gives us fraction of tuples in $S$.

\subsubsection{Estimating the size of join}
\label{join}
We start with natural join. Let's have expression $A=R(X,Y)\Join S(Y,Z)$. We join here by one attribute and we use estimation:

\begin{itemize}
\item $T(A)=\dfrac{T(R)T(S)}{max(V(R,Y),V(S,Y))}$
\end{itemize}

We can generalize this rule for joining with multiple attributes. For join $R\Join S$, where we join $R$ and $S$ using following attributes $a_1,a_2,...a_n)$, we use this estimation:

\begin{itemize}
\item $T(R\Join S)=\dfrac{T(R)T(S)}{\prod_{k=1}^{n}{max(V(R,a_k),V(S,a_k))}}$
\end{itemize}

When we join using multiple attributes, than result of this estimation can be smaller than $1$. This estimation indicates that relation will be very small or possibly empty.

If we have other type of join (theta join), we can use following rules for it's estimation:
\begin{enumerate}
\item Size of product is product of sizes of relations involved.
\item Equality conditions can be estimated using techniques presented in natural joins.
\item An inequality compassion can be handled like inequality comparison in expression $\sigma_{A<c}(R)$. We assume that $1/3$ of tuples will satisfy condition. 
\end{enumerate}

\subsubsection{Estimating the size of union}

If we have bag union, then size of resulting relation is sum of sizes of input relations. Size of set union $R \cup_S S$ can be between $max(T(R),T(S))$ and $T(R)+T(S)$. Recommended estimate is average of this numbers. 

\subsubsection{Estimating the size of intersection}
Size of relations $R\cup_S S$ and $R\cup_B S$ can be between $0$ and $min(T(R),T(S))$. Recommend estimate is to take half of size of smaller relation:
\begin{itemize}
\item  $min(T(R),T(S))/2$.
\end{itemize}

\subsubsection{Estimating the size of difference}

Result of expression $R-S$ can be as big as $T(R)$ and as small as $T(R)-T(S)$. Suggested estimate can be used for bag or set version of difference: 
\begin{itemize}
\item  $T(R-S)=T(R)-\frac{T(S)}{2}$.
\end{itemize}
\subsubsection{Estimating the size of grouping}
Size of the result of expression $\gamma_L(R)$ is $V(R,[g_1,g_2,...,g_n])$, where $g_x$ are grouping attributes of $L$. This statistics is almost never available so we need another estimate. $T(\gamma_L(R))$ can be between 1 and $\prod_{k=1}^{n}{V(R,g_k)}$. Upper bound can be larger than number of tuples in relation $R$. That's why we suggest:
\begin{itemize}
\item $T(\gamma_L(R))=min(\dfrac{T(R)}{2},\prod_{k=1}^{n}{V(R,g_k)})$
\end{itemize}
Estimation of duplicate elimination can be handled exactly like grouping.

\subsection{Enumerating plans}
For every logical plan there is exponential many physical plans. in this section we present ways to enumerate physical plans, so we can choose plan with least estimated cost. There are two broad approaches:
\begin{itemize}
\item Top-down: We go down the algebra tree. For each possible implementation of operation, we consider best possible algorithms for it's subtrees. We compute costs of every combination and take the best.
\item Bottom-up: We go up the tree and for every subexpression we enumerate plans based on plans generated for it's subexpressions.
\end{itemize}

There is not much difference between this approaches but one method eliminates plans that other method can't and vice versa.
\subsubsection{Heuristic selection}

We use same approach for generating physical plan as we used to improve logical plan. We make choices based on heuristics. Here are some heuristics that can be used:

\begin{itemize}

\item If an join attribute has an index, we can use joining by index.
\item If one argument of join is sorted, then we prefer merge join to hash join.
\item If we compute intersection of union of more than 2 relations, we perform algorithm on smaller relations first.
\end{itemize}

\subsubsection{Branch-and-bound plan enumeration}
Branch-and-bound plan enumeration is ofter use in practice. We begin by finding physical plan using heuristics. We denote cost of this plan $C$. Then we can consider other plans for sub-queries. We can eliminate any plan for sub-query with cost greater than $C$. If we get plan with lower estimated cost than $C$ we use this plan instead.

The great advantage is we can choose when to cut search for better plan. If $C$ is low then we don't continue searching other plan. On the other hand when $C$ is large, it is better to invest some time in finding faster plan.
\subsubsection{Hill climbing}

First we start with heuristically selected plan. Then we try to do some changes, for example changing order of joins or replacing operator using hash table for sort-based operator. When we find a plan where no small modification give up better plan, then we make this plan out chosen physical plan.

\subsubsection{Dynamic programming}
Dynamic programming is variation of bottom-up strategy. For each subexpression we keep only one plan of least cost.

\subsubsection{Selinger-Style Optimization}
This is a improvement of dynamic programing approach. We keep for every subexpression not only best plan, but also other plans with higher cost, which result is in some way sorted. This might be useful in following situation:
\begin{enumerate}
\item The sort operator $\tau_L$ is on the root of tree and we have plan, which is sorted by some or all attributes in $L$.
\item Plan is sorted by attribute used later in grouping.
\item We are joining by some sorted attribute.
\end{enumerate}
In this situation we don't have to sort input or we need only partial sort and we can use faster algorithm, which takes advantage of sorted input.


\subsection{Choosing join order}
\label{joinOrder}
 We have three choices how to choose order of join of multiple relation:
\begin{enumerate}
\item Consider all possible options.
\item Consider only subset of join orders.
\item Use heuristic to pick one.
\end{enumerate}
 
In this section we present algorithm that can be used for choosing join order.
 
\subsubsection{Dynamic programing to select a join order}
\label{dymanicalgorithm}
For this algorithm we need a table, where we store following informations:
 
\begin{enumerate}
\item Estimated size of relation.
\item Cost to compute current relations.
\item Expression how was current relation computed.
\end{enumerate}

We store every single input relation with estimated cost 0. For every pair of relations we compute their estimated size of join and store it into table. Fo that we use estimation describe in section \ref{join}.  

After that we compute entry of all subset of sizes $(3,4,...n)$.
If we want to consider all possible trees, we need to partition relations in join $R$ into two non empty disjoint sets. For each pair of sets we compute estimated size and cost of their join to get join $R$. For this we use data already in our table. We are estimating join of $k$ relations and all joins of $k-1$ relations are already estimated. The join tree with least estimated cost will be stored in the table.

Example: for estimating join $A\Join B\Join C\Join D$ we try to join following subsets:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\item $A\Join B$ and $C\Join D$
\item $A\Join C$ and $B\Join D$
\item $A\Join D$ and $B\Join C$

\end{enumerate}

We don't have to consider all trees but only so called left-deep join tree. Tree is called left-deep tree if all of it's right children are leafs. Right-deep tree is tree, where all left children are leafs.
 
We do it the same way like we wanted to include all possible trees, but with one exception. When partitioning $R$ into two non empty disjoint sets, we make sure that one set has only one relation.

For estimating join $A\Join B\Join C\Join D$ we try to join following subsets:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\end{enumerate}

\subsubsection{Greedy algorithm for selecting a join order}
\label{greedyalgorithm}
Time complexity of using dynamic programming to select an order of join is exponential. We can us it for small amount of relations. If we don't want to invest time we can use greedy algorithm. This algorithm has to store same information as dynamic programing algorithm for every relation.

We start with a pair fo relations $R_i, R_j$, for which size of $R_i \Join R_j$ is smallest. This join is our current tree.

For other relation that are not yet included we find relation $R_k$, so that it's join with current tree give us smallest estimated size. We continue until we include all relations into tree. 

This algorithm will also create left-deep or right-deep join tree. There are examples where dynamic algorithm finds plans with lower estimated cost than greedy algorithm.
 
\subsection{Choosing physical algorithms}

To complete physical plan we need to assign physical algorithms to operations in logical plan.
\subsubsection{Choosing selection algorithms}

Basic approach is to use index on relation instead of reading whole relation. We can usel following heuristic for picking selection algorithm: 
\begin{itemize}
\item If there is selection $\sigma_{A\oplus c}$ and relation $R$ has index on attribute $A$, $c$ is constant and $\oplus$ is $=$, $<$, $\leq$, $>$ or $\geq$, then we use scanning by index instead of scanning table with filtering result.
\item More general rule if selection contains condition $A\oplus c$ and selected relation contains index of $A$, then we use scanning by index and filtering result based on other parts of condition.
\end{itemize}

\subsubsection{Choosing join algorithms}

We recommend to use sort join when:

\begin{enumerate}
\item On or both join relations are sorted by join attributes.
\item There are more join on the same attributes. For join $R(a,b)\Join S(a,c) \Join T(a,d)$ we can join first $R$ and $S$ by sort based algorithm. We can assume that $R \Join S$ will be sorted by attribute $a$ and then we use second sort join.

\end{enumerate}

If we have join $R(a,b)\Join S(b,c)$ and we expect size of relation to be small and $S$ have index on attribute on $b$, we can use join algorithm using this index.

If there are no sorted relation and we don't have any indexes on relations, it is probably best option to use hash base algorithm.

\subsubsection{Choosing scanning algorithms}

Leaf of relation tree will be replaced by scanning operator:

\begin{itemize}
\item $TableScan(R)$ - operator reads entire table.

\item $SortScan(R,L)$ - operator reads entire table and sort by attributes in list~$L$.

\item $IndexScan(R,C)$ - $C$ is a condition in form $A\oplus c$, where $c$ is constant, $A$ is attribute and $\oplus$ is $=$, $<$, $\leq$, $>$, $\geq$. Operator reads tuples satisfying condition using index on $A$, result will be sorted by columns on used index.

\item $TableScan(R,A)$ - $A$ is an attribute. Operator reads entire table using index on $A$, result will be sorted by columns on used index.

\end{itemize}

We choose scan algorithm based of need of sorted output and availability of indexes.

\subsubsection{Other algorithms}

Basically for other operators we usually have sort and hash version of algorithm. For example for processing grouping we can create groups using hash table or we can sort relation by grouped operators. We can choose:

\begin{itemize}
\item Hash version of algorithm if the input is not sorted in way we need or if the output doesn't have to be sorted.

\item Sort version of algorithm if we have sorted input by some of requested parameters, or we need sorted input. In case the input is only sorted by some needed attributes we can still use pretty fast partial sort and apply sort based algorithm.

\end{itemize}