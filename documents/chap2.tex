\chapter{Related work}
I the chapter we introduce some theory of relational algebra, it's optimizations and physical plan generation. This informations were used for tool implementation.
\section{Relational algebra}

In this chapter we introduce and describe relational algebra\cite{database}. We start with some basic definitions of relational model.

\begin{mydef}
$Relation$ is a two dimensional table.
\end{mydef}
\begin{mydef}
$Attribute$ is column of a table.
\end{mydef}
\begin{mydef}
$Schema$ name of the relations and a set of attributes. For example:~$Movie(id,name,lenght)$.
\end{mydef}
\begin{mydef}
$Tupple$ of a relation is a row other than header row.
\end{mydef}


An algebra in general consist of  operators and atomic operands. For example in arithmetic algebra variables like $x$ or constant like 15 and operators are addition multiplication, subtraction and division.
We can build expression by applying operators on operands or other expressions. Example of an expression in arithmetic algebra is $(15+x)*x$.

Relational algebra  has atomic operands:
\begin{itemize}
\item Variables, that are relations.
\item Constants, that are finite relations. 
\end{itemize}

In classical relational algebra all operates and expression results are set. All this operations can be applied also to bags.
Relation algebra operators are:
\begin{itemize}
\item Set operations - union, difference, intersection.
\item Removing operators - selection, which removes rows and projection that eliminates columns from given relation.
\item Operations that combine two relation, all kinds of joins.
\item Renaming operations, that doesn't change tuples of the relation bud changes schema.
\end{itemize}
Expressions in relational algebra are called $queries$.
\subsection{Classical relational algebra operators}
\subsubsection{Set operations on relations}

Sets operations are:
\begin{itemize}
\item Union $R\cap S$ is a set of tuples that are in $R$ or $S$.
\item Intersection $R\cup S$  is a set of tuples that are in both $R$ and $S$. 
\item Difference $R-S$ is a set of tuples that are in $R$ but not in $S$.

\end{itemize}

Lets have relations $R$ and $S$. If we want to apply some set operation both relations must have the same set of attributes. If we want to compute set theoretic union, difference or intersections the oder of columns must be the same in both relations. 
We can also use renaming operations if relations doesn't have same number of attributes.

\subsubsection{Projection}
\label{projection}
$Projection$ operator $\pi$ produces from relation R new Relations with reduced set of attributes. Result of a expression $\pi_{A_1,A_3,A_4,...,A_N}(R)$ is relation $R$ with attributes $A_1,A_3,A_4,...,A_N$. 

\subsubsection{Selection}
If we apply operator selection $\sigma$ on Relation $R$ with condition $C$ we get a new relation with same attributes and tuples, which satisfy given condition. For example $\sigma_{A_1=4}(R)$.

\subsubsection{Cartesian product}
Cartesian product of two sets $R$ and $S$ creates a set of pairs by choosing the first element of pair to be any element from R and second element of pair to be any element of S. Cartesian product of relations similar. We pair tuples from $R$ with all tuples from $S$.

\subsubsection{Natural joins}
We usually don't want to pair all of the tuples from $R$ to all tuples from $S$. We can par tuple in some other way. The simples join is called natural join of $R$ and $S$ ($R \Join S$). Let schema of $R$ be $R(r_1,r_2,..,r_n,c_1,c_2,...c_n)$ and schema of $S$ be  $S(s_1,s_2,..,s_n,c_1,c_2,...c_n)$. In natural join we pair tuple $r$ from relation $R$ to tuple $s$ from relation $S$ only if $r$ and $s$ agree on all attributes with same name (in this case $c_1,c_2,...c_n$).

\subsubsection{Theta joins}
Natural join forces us to use one specific condition. In may cases we want to join relation with some other condition. For this purpose we have theta-join. The notation for joining relation $R$ and $S$ based on condition $C$ is $R\Join_C S$. The result is constructed in following way:

\begin{enumerate}
\item Make Cartesian product of $R$ and $S$
\item Use selection with condition $C$.
\end{enumerate}

Basically $R\Join_C S=\sigma_C(R \times S)$

\subsubsection{Renaming}

In order to control name of attributes or relation name we have renaming operator. We can use operator $\rho_{S(A_1,A_2,...,A_n)}(R)$. Result will have the same tuples as $R$ but relation will be called S and attributes will be renamed to $(A_1,A_2,...,A_n)$.

\subsection{Relational operations on bags}

Commercial database system almost never are based purely on bags. $Bag$ is a multi-set. Only operation that behave differently are intersection union and difference. 

\subsubsection{Union}
Bag union of $R \cup S$ we just add all tuples from $S$ and $R$ together. If tuple $t$ appears in $R$ $m$-times and in $S$ $n$-times then in  $R \cup S$ will $t$ appear $m+n$ time. Both $m$ and $n$ can be zero.

\subsubsection{Intersection}

Lets have tuple $t$ that appears in $R$ $m$-times and $S$ $n$-times. In the Bag intersection $R \cap S$ will be $t$ $min(m,n)$-times.

\subsubsection{Difference}
Every tuple $t$ that appears in $R$ $m$-times and $S$ $n$-times, will appear $max(0,m-n)$ times in bag $R-S$.


\subsection{Extended operators of Relational algebra}

We will introduce extended operators that proved useful in many query languages like SQL.


\subsubsection{Duplicate elimination}
This operator $\delta(R)$ returns set consisting of one copy of every tuple that appears in bag R one or more times.

\subsubsection{Aggregate operations}

Aggregate operators such as sum are not relational algebra operator but are used  by grouping operator. They apply on column and produce one number as result. The standard operators are $SUM$, $AVG$(average), $MIN$, $MAX$ and $COUNT$.


\subsubsection{Grouping operator}

We often doesn't want to compute aggregation function for entire column. We rather compute this function on for some group of columns. For example we can compute average salary for every person in database, or we can group them by companies and get every salary in every company. 

For this purpose we have grouping operator $\gamma_L(R)$. $L$ is a list of:

\begin{enumerate}
\item Attribute of $R$ by which $R$ will be grouped.
\item Aggregation operator applied on a attribute of relation.
\end{enumerate}

Relation computed by expression $\gamma_L(R)$ is constructed:

\begin{enumerate}
\item Relation will be partitioned into groups. Every group contains all tuples which have same value in all grouping attributes. If there is no grouping attributes, all tuples will be in one group.
\item For each group operator produces one tuple consisting of:
 \begin{enumerate}
 	\item Grouping attributes values for group.
 	\item Results of aggregations over all tuple of processed group.
 \end{enumerate}
\end{enumerate}

Duplicate elimination operator is a special case of grouping operator. We can express $\delta(R)$ with $\gamma_{L}(R)$, where $L$ is a list of all attributes of $R$.

\subsubsection{Extended projection operator}

We can extend classical projection operator $\pi_L(R)$ introduced in chapter \ref{projection}. We denote it also $\pi_L(R)$ but projection list can have following elements:

\begin{enumerate}
\item Attribute of R, which means attribute will appear in output.
\item Expression $x = y$, attribute $y$ will be renamed to $x$.
\item Expression $x = E$, where $E$ is an expression created from attributes from R, constants, arithmetic, string  and other operators. $x$ is new name. For example $x=e*(1-l)$.
\end{enumerate}



\subsubsection{The sorting operator}

In several situations we want the output of query to be sorted. Expression $\tau_L(R)$, where $R$ is relation, $L$ is list of attributes with additional information about sort order, is relation with same tuples like $R$ but different order of tuples. Example:  $\tau_{A_1:A,A_2:D}(R)$ will sort relation $R$ by attribute $A_1$ ascending and tuples with same $A_1$ value will be additionally sorted by their $A_2$ value descending. 

\subsubsection{Outer joins}
Lets have join $R\Join_C S$. We call tuple $t$ from relation $R$ or $S$ $dangling$ if
we didn't find any match in relation $S$ or $R$. Outer join $R\Join^\circ_C S$ is formed by creating $R\Join_C S$ and adding dangling tuples from $R$ and $S$. The added tuples must be filled with special $null$ value in all attributes they don't have but appear in join result.

Left/right outer join is outer join but we only add dangling tuples from left/right relation.

\section{Optimizations of relational algebra}

After initial logical query plan is generated, we can apply some heuristics to improve it, using some algebraic laws that hold for relational algebra. 


\subsection{Commutative and associative laws}
Commutative and associative operators are Cartesian product, natural join, union and intersection. Theta join is commutative but generally is not associative. But if the conditions makes sense where they where positioned, then theta join is associative.
That means we can make following changes to algebra tree:

\begin{itemize}
\item $R \oplus S = S \oplus R$

\item $(R \oplus S) \oplus T = R \oplus (S \oplus T)$
\end{itemize}
$\oplus$ stands for $\times$, $\cap$, $\cup$, $\Join$ or $\Join_C$.

\subsection{Laws involving selection}

Selection are very important for improving logical plan. They usually reduce size of relation markedly so that's why we need to move them down the tree as far as possible.
We can change order of selections:

\begin{itemize}
\item $\sigma_{C_1}(\sigma_C{_2}(R)) = \sigma_C{_2}(\sigma_C{_1}(R))$
\end{itemize}
Sometimes we cannot push whole condition but we can split it:

\begin{itemize}
\item $\sigma_{C_1~AND~C_2}(R)=\sigma_{C_1}(\sigma_{C_2}(R))$

\item $\sigma_{C_1~OR~C_2}(R)=\sigma_{C_1}(R) \cup_S \sigma_{C_2}(R)$
\end{itemize}
Last law works only when $R$ is a set. $\cup_S$ stands for set union. We can push selection down union, it has to be pushed to both branches:
\begin{itemize}
\item $\sigma_{C}(R \cup S)=\sigma_{C}(R) \cup \sigma_{C}(S)$
\end{itemize}

When pushing selection through difference we must push it to first branch. Pushing to second branch optional. Laws for difference: 
\begin{itemize}
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - \sigma_{C}(S)$
\item $\sigma_{C}(R-S)=\sigma_{C}(R) - S$
\end{itemize}

Following laws allow to push selection down both arguments. Let's have selection $\sigma_C$. We can push it to the branch, which contains all attributes used in $C$. If $C$ contains only attributes of $R$:
\begin{itemize}
\item $\sigma_{C}(R \oplus S)=\sigma_{C}(R) \oplus S$
\end{itemize}
$\oplus$ stands for $\times$, $\cap$, $\cup$, $\Join$ or $\Join_C$. If relation $S$ and $R$ contains all attributes of $C$ we can also use following law:
\begin{itemize}
\item $\sigma_{C}(R \Join S)=\sigma_{C}(R) \Join  \sigma_{C}(S)$
\end{itemize}


\subsection{Laws involving projection}
Principle for manipulation with projections that we can add projection anywhere in the tree as long as it only eliminates attributes which are not used anymore and don't appear in query result.
\subsection{Laws involving joins and products}

We have more laws involving selection that follow directly from definition of the join:
\begin{itemize}
\item $\sigma_{C}(R \times S)=R \Join_{C} S$
\item $R \Join S=\pi_L(\sigma_{C}(R \times S))$, $C$ is condition that equates each pair of attributes of $R$ and $S$, which have the same name and $L$ is a list of attributes of relation $R$.
\end{itemize}



\section{Physical plan generation}

After we optimized logical plan, we need to create physical plan. We generate many physical plans a choose one with least estimated cost to run it. This approach is called cost-based enumeration.

For each physical plan we select 
\begin{enumerate}
\item An order of grouping and joins.
\item An algorithm for each operator. For example if we use join based on hashing or sorting.
\item Additional operators which are not presented in logical plan. For example we can sort relation in order to user faster algorithm which assumes that it's input is sorted.
\item The way in which arguments are pass to between operators. We can use iterators for it or store result on hard drive.
\end{enumerate}

\subsection{Size estimations}

The costs of evaluating physical plan are based of estimated size of interme\-dia\-te relations. Ideally we want out estimation to be accurate, easy to compute and logically consistent(size of relation doesn't depend on how relation is computed). We will present simple rules, which will give us give us good estimations in most situation. Goal of estimating sizes is not predict exact size of relation, even an inaccurate sizes will help us with plan generation.

In this section we will use following conventions:

\begin{itemize}
\item $T(R)$ is number of tuples in relation R.
\item $V(R,a)$ number of distinct values in attribute $a$. 
\item $V(R,[a_1,a_2,...,a_n])$ is number of tuples in $\delta(\pi_{a_1,a_2,...,a_n}(R))$
\end{itemize}

\subsubsection{Estimating the size of projection}

Projection is only operator which size of result is computable. It doesn't change number ot tuples, only their lengths change.

\subsubsection{Estimating the size of selection}

Selection usually reduces number of tuples. Lets have $S=\sigma_{A=c}(R)$, where $A$ is a attribute of $R$ and $c$ is a constant. Recommended estimation is:
\begin{itemize}
\item $T(S)=T(R)/V(R,A)$
\end{itemize}

More problematic estimation is when selection involves inequality comparison Lets have $S=\sigma_{A<c}(R)$. In average half the tuple satisfies condition, but usually queries select only a small fraction from all tuples. Therefore the estimation is:
\begin{itemize}
\item $T(S)=T(R)/3$
\end{itemize}

For selection where condition is in form $C_1~and~C_2~and~...~and~C_N$ we can treat selection as a cascade of simple selections and estimate size for every simpler condition.

In case we have following condition: $S=\sigma_{not(C)}(R)$, we use this estimation:
\begin{itemize}
\item $T(S)=T(R)-T(\sigma_C(R))$
\end{itemize}

Little more complicated is when condition involved an $or$ of conditions. Lets have expression $S=\sigma_{C_1~or~C_2}(R)$. We assume that $C_1$ and $C_2$ are independent. Size of $S$ is:

\begin{itemize}
\item $T(S)=T(R)(1-(1-\dfrac{m_1}{T(R)})(1-\dfrac{m_2}{T(R)}))$
\end{itemize}

Expression $1-\dfrac{m_1}{T(R)}$ is fraction of tuples which doesn't satisfy condition $C_1$ and $1-\dfrac{m_2}{T(R)}$ is fraction of tuples which doesn't satisfy condition $C_2$. Product of these numbers are the fraction of tuples from $R$ which are not in result. One minus the product gives us fraction of tuples in $S$.

\subsubsection{Estimating the size of join}
\label{join}
We start with natural join. Let's have expression $X=R(X,Y)\Join S(Y,Z)$. We join here by one attribute and we use estimation:

\begin{itemize}
\item $T(X)=\dfrac{T(R)T(S)}{max(V(R,Y),V(S,Y))}$
\end{itemize}

We can generalize this rule for joining with multiple attributes. For join $R\Join S$, where we join $R$ and $S$ using following attributes $a_1,a_2,...a_n)$, we use this estimation:

\begin{itemize}
\item $T(R\Join S)=\dfrac{T(R)T(S)}{\prod_{k=1}^{n}{max(V(R,a_k),V(S,a_k))}}$
\end{itemize}

 

If we have other type of join (theta join), we can use following rules for it's estimation:
\begin{enumerate}
\item Size of product is product of sizes of relations involved.
\item Equality conditions can be estimated using techniques presented in natural joins.
\item An inequality compassion can be handled like inequality comparison in expression $\sigma_{A<c}(R)$. We assume that $1/3$ of tuples will satisfy condition. 
\end{enumerate}

\subsubsection{Estimating the size of union}

If we have bag union, then size of resulting relation is sum of sizes of input relations. Size of set union $R \cup_S S$ can be between $max(T(R),T(S))$ and $T(R)+T(S)$. Recommended estimate is average of this numbers. 

\subsubsection{Estimating the size of intersection}
Size of relation $R\cup S$ can be between $0$ and $min(T(R),T(S))$. We recommend to take half of size of smaller relation:
\begin{itemize}
\item  $min(T(R),T(S))/2$.
\end{itemize}

\subsubsection{Estimating the size of difference}

Result of expression $R-S$ can be as big as $T(R)$ and as small as $T(R)-T(S)$. Suggested estimate is: 
\begin{itemize}
\item  $T(R-S)=T(R)-T(S)$.
\end{itemize}
\subsubsection{Estimating the size of grouping}
Size of the result of expression $\gamma_L(R)$ is $V(R,[g_1,g_2,...,g_n])$, where $g_x$ are grouping attributes of $L$. This statistics is almost never available so we need another estimate. $T(\gamma_L(R))$ can be between 1 and $\prod_{k=1}^{n}{V(R,g_k)}$. Upper bound can larger than number of tuple in relation $R$. That's why we suggest:
\begin{itemize}
\item $T(\gamma_L(R))=min(\dfrac{T(R)}{2},\prod_{k=1}^{n}{V(R,g_k)})$
\end{itemize}
Duplicate elimination can be handled exactly like grouping.

\subsection{Enumerating plans}
For every logical plan there is exponential many physical plans. in this section we present way to enumerate physical plans so we can choose plan with least estimated cost. There are two broad approaches:
\begin{itemize}
\item Top-down: We go down the algebra tree. For each possible implementation of operation, we consider best possible algorithms for it's subtrees. We compute costs of every combination and take the best.
\item Bottom-up: We go up the tree and for every subexpression we enumerate plans based on plans generated for it's subexpressions.
\end{itemize}

There is not much difference between this approaches but one method eliminates plans that other method can't and vice versa.
\subsubsection{Heuristic selection}

We use same approach for generating physical plan as we used to improve logical plan. We make choices based on heuristics. Here are some heuristics that can be used:

\begin{itemize}

\item If an join attribute has an index, we can use joining by index.
\item If one argument of join is sorted, then we prefer merge join to hash join.
\item If we compute intersection of union of more than 2 relations, we perform algorithm on smaller relations first.
\end{itemize}

\subsubsection{Branch-and-bound plan enumeration}
Branch-and-bound plan enumeration is ofter use in practice. We begin by finding physical plan using heuristics. We denote cost of this plan $C$. Then we can consider other plans for sub-queries. We can eliminate any plan for sub-query with cost greater than $C$. If we get plan with lower estimated cost than $C$ we use this plan instead.

The grate advantage is we can choose when to cut search for better plan. If $C$ is low then we don't continue searching other plan. On the other hand when $C$ is large, it is better to invest some time in finding a  better plan.
\subsubsection{Hill climbing}

First we start with heuristically selected plan. Then we try to do some changes, for example changing order of joins or replacing operator using hash table for sort-based operator. When we find a plan where no small modification give up better plan, then we make this plan out chosen physical plan.

\subsubsection{Dynamic programming}
Dynamic programming is variation of bottom-up strategy. For each subexpression we keep only one plan of least cost.

\subsubsection{Selinger-Style Optimization}
This is a improvement of dynamic programing approach. We keep for every subexpression not only best plan, but also other plans with higher cost, which result is in some way sorted. This might be useful in following situation:
\begin{enumerate}
\item The sort operator $\tau_L$ is on the root of tree and we have plan, which is sorted by some or all attributes in $L$.
\item Plan is sorted by attribute used later in grouping.
\item We are joining by some sorted attribute.
\end{enumerate}
In this situation we don't have to sort input or we need only partial sort and we can use faster algorithm, which takes advantage of sorted input.


\subsection{Choosing join order}
\label{joinOrder}
 We have three choices how to choose order of join of multiple relation:
\begin{enumerate}
\item Consider all possible options.
\item Consider only subset of join orders.
\item Use heuristic to pick one.
\end{enumerate}
 
In this section we present algorithm that can be used for choosing join order.
 
\subsubsection{Dynamic programing to select a join order}
 
For this algorithm we need a table, where we store information following information:
 
\begin{enumerate}
\item Estimated size of relation.
\item Cost to compute current relations.
\item Expression how was current relation computed.
\end{enumerate}

We store every single relation $R$ with estimated cost 0. For every pair of relations we compute their estimated size of join and store it into table. Fo that we use estimation describe in section \ref{join}.  

After that we compute entry of all subset of sizes $(3,4,...n)$.
If we want to consider all possible trees, we need to partition relations in join $R$ into two non empty disjoint sets. For each pair of sets we compute estimated size and cost of their join to get join $R$. For this we use data already in our table. We are estimating join of $k$ relations and all joins of $k-1$ relations are already estimated. The join tree with least estimated cost will be stored in the table.

Example: for estimating join $A\Join B\Join C\Join D$ we try to join following subsets:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\item $A\Join B$ and $C\Join D$
\item $A\Join C$ and $B\Join D$
\item $A\Join D$ and $B\Join C$

\end{enumerate}

We don't have to consider all trees but only so called left-deep join tree. Tree is called left-deep tree if all of it's right children are leafs. Right-deep tree is tree, where all left children are leafs.
 
We do it the same way like we wanted to include all possible trees, but with one exception. When partitioning $R$ into two non empty disjoint sets, we make sure that one set has only one relation.

Example: for estimating join $A\Join B\Join C\Join D$ we try to join following subsets:
\begin{enumerate}
\item $A$ and $B\Join C\Join D$
\item $B$ and $A\Join C\Join D$
\item $C$ and $A\Join B\Join D$
\item $D$ and $A\Join B\Join C$
\end{enumerate}

\subsubsection{Greedy algorithm for selecting a join order}

Time complexity of using dynamic programming to select an order of join is exponential. We can us it for small amount of relations (maybe maximal 5 or 6).If we don't want to invest time we can use greedy algorithm. This algorithm has to store same information as dynamic programing algorithm for every relation.

We start with a pair fo relations $R_i, R_j$, for which size of $R_i \Join R_j$ is smallest. This join is our current tree.

For other relation that are not yet included we find relation $R_k$, so that it's join with current tree give us smallest estimated size. We continue until we include all relations into tree. 

This algorithm will also create left-deep or right-deep join tree.

There are examples where dynamic algorithm finds plans with lower estimated cost than greedy algorithm.
 
\subsection{Choosing physical algorithms}

To complete physical plan we need to assign physical algorithms to operations in logical plan.
\subsubsection{Choosing selection algorithms}

Basic approach is to use index on relation instead of reading whole relation. We can usel following heuristic for picking selection algorithm: 
\begin{itemize}
\item If there is selection $\sigma_{A\oplus c}$ and relation $R$ has index on attribute $A$, $c$ is constant and $\oplus$ is $=$, $<$, $\leq$, $>$ or $\geq$, then we use scanning by index instead of scanning table with filtering result.
\item More general rule if selection contains condition $A\oplus c$ and selected relation contains index of $A$, then we use scanning by index and filtering result based on other parts of condition.
\end{itemize}

\subsubsection{Choosing join algorithms}

We recommend to use sort join when:

\begin{enumerate}
\item On or both join relations are sorted by join attributes.
\item There are more join on the same attributes. For join $R(a,b)\Join S(a,c) \Join T(a,d)$ we can join first $R$ and $S$ by sort based algorithm. We can assume that $R \Join S$ will be sorted by attribute $a$ and then we use second sort join.

\end{enumerate}

If we have join $R(a,b)\Join S(b,c)$ and we expect size of relation to be small and $S$ have index on attribute on $b$, we can use join algorithm using this index.

If there are no sorted relation and we don't have any indexes on relations, it is probably best option to use hash base algorithm.

\subsubsection{Choosing scanning algorithms}

Leaf of relation tree will be replaced by scanning operator:

\begin{itemize}
\item $TableScan(R)$ - operator reads entire table.

\item $SortScan(R,L)$ - operator reads entire table and sort by attributes in list~$L$.

\item $IndexScan(R,C)$ - $C$ is a condition in form $A\oplus c$, where $c$ is constant, $A$ is attribute and $\oplus$ is $=$, $<$, $\leq$, $>$, $\geq$. Operator reads tuples satisfying condition using index on $A$, result will be sorted by columns on used index.

\item $TableScan(R,A)$ - $A$ is an attribute. Operator reads entire table using index on $A$, result will be sorted by columns on used index.

\end{itemize}

We choose scan algorithm based of need of sorted output and availability of indexes.

\subsubsection{Other algorithms}

Basically for other operators we usually have sort and hash version of algorithm. For example for processing grouping we can create groups using hash table or we can sort relation by grouped operators. We can choose:

\begin{itemize}
\item Hash version of algorithm if the input is not sorted in way we need or if the output doesn't have to be sorted.

\item Sort version of algorithm if we have sorted input by some of requested parameters, or we need sorted input. In case the input is only sorted by some needed attributes we can still use pretty fast partial sort and apply sort based algorithm.

\end{itemize}